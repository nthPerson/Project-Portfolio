<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Unitree Go1 Software Package">
    <title>Unitree Go1 Software Package</title>
    <link rel="stylesheet" href="../bootstrap.min.sandstone.css">
    <style>
        .navbar-brand {
            font-size: 1.75rem;
            font-weight: bold;
        }
        .navbar-subtitle {
            font-size: 1rem;
            color: rgba(255, 255, 255, 0.8);
            display: block;
        }
        .btn-success, .btn-warning {
            margin: 0 10px;
        }
        img, video {
            display: block;
            margin: 20px auto;
            border-radius: 0.5rem;
            max-width: 66%;
        }
        figure {
            text-align: center;
        }
        figure figcaption {
            margin-top: 5px;
            font-style: italic;
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-lg bg-dark" data-bs-theme="dark">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">Robert Ashe - Project Portfolio
                <span class="navbar-subtitle">Big Data Analytics M.S., San Diego State University</span>
            </a>
        </div>
    </nav>
    <main class="container my-5">
        <h1 class="text-center">Unitree Go1 Software Package</h1>
        <p>The Go1 Software Package is a modular, high-level control framework developed for Unitree robots. Designed at the DiCE Lab at San Diego State University, this framework bridges robotics, Building Information Models (BIM), and Unity-based simulations, simplifying mission planning and enabling seamless integration with sensors, edge devices, and APIs. The system includes a construction-specific implementation for Human Activity Recognition (HAR) using machine learning, enhancing trust in human-robot collaboration through real-time inference and explainability.</p>
        <div class="text-center">
            <a href="https://github.com/nthPerson/UnitreeGO1-HRI-BIM" class="btn btn-success" target="_blank">GitHub Repository</a>
            <a href="https://drive.google.com/drive/folders/1wp_nPnqQrEC_u9uZVlHhcJBwAp9au4O_?usp=sharing" class="btn btn-warning" target="_blank">Image Gallery</a>
        </div>
        <h2 class="mt-5">My Role</h2>
        <p>As a research assistant at the DiCE Lab, I contributed to the development and implementation of this project, and I was the primary author for the Explainable AI (XAI) feature for HAR. My responsibilities included designing core components of the Python backend, integrating client-server communication, and enhancing transparency through natural language explanations generated by OpenAI's GPT model.</p>
        <figure>
            <img src="../assets/go1_electronics_mount_process_1.jpg" alt="Go1 and post-processing equipment">
            <figcaption>Go1 and 3D printing post processing equipment during the creation of the mounting solution for the custom electronics used for human activity recognition.</figcaption>
        </figure>
        <h2>Technical Details</h2>
        <ul>
            <li><strong>Explainable AI Feature:</strong>
                <ul>
                    <li>Developed the explain_processor.py module to generate detailed explanations of activity inferences.</li>
                    <li>Implemented a pipeline that gathers inference data, including video frames and model outputs, and uses OpenAI's API to provide natural language explanations.</li>
                    <li>Enabled traceability by saving generated explanations alongside auto-labeled data.</li>
                </ul>
            </li>
            <li><strong>Client-Server Architecture:</strong>
                <ul>
                    <li>Modular Python backend with TCP-based client-server communication.</li>
                    <li>Facilitates real-time data exchange between Unity, the Python backend, and the robotic vehicle.</li>
                </ul>
            </li>
            <li><strong>BIM and Unity Integration:</strong>
                <ul>
                    <li>Leverages BIM data for pathfinding and mission planning in Unity.</li>
                    <li>Enables high-fidelity simulation of the robot's environment to facilitate a wide variety of applications.</li>
                </ul>
            </li>
        </ul>
        <figure>
            <img src="../assets/go1_electronics_mount_process_6.jpg" alt="Raspberry Pi serving video streams">
            <figcaption>A view of the Raspberry Pi that serves the video stream to the local server that enables human activity recognition.</figcaption>
        </figure>
        <h2>Challenges and Solutions</h2>
        <p>Scaling up production for custom hardware mounts and ensuring real-time processing efficiency presented significant challenges. These were overcome by optimizing manufacturing processes and employing efficient data streaming techniques. Other challenges included:</p>
        <ul>
            <li><strong>Dataset Scarcity:</strong> Construction-specific HAR datasets are limited. To address this, I helped develop a self-labeling mechanism that dynamically generates labeled data during HAR, demonstrating the potential for semi-supervised learning to streamline dataset creation.</li>
            <li><strong>Explainability in AI:</strong> Increasing transparency is crucial to promoting trust in human-robot collaboration. By integrating the OpenAI API, I ensured that the system could generate clear, context-aware explanations for all inferences made during HAR.</li>
        </ul>
        <figure>
            <video controls>
                <source src="../assets/slow_XAI_test.MOV" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Initial testing of the explainable AI (XAI) feature during development. A researcher (Robert Ashe) swings a hammer to simulate a construction task, and the system responds with an explanation of the inference.</figcaption>
        </figure>
        <h2>Outcome and Impact</h2>
        <ul>
            <li>Enhanced accessibility of robotics for non-experts by simplifying sensor integration and customization.</li>
            <li>Improved human-robot collaboration through real-time HAR and explainable AI.</li>
            <li>Open-source availability of the package supports adoption and customization by researchers across various industries.</li>
        </ul>
        <h2>Why Include This Project?</h2>
        <p>This project exemplifies my ability to collaborate using machine learning, robotics, and software engineering to address practical problems.  It highlights my skills in Python development, explainable AI, and experience with modular systems that facilitate real-world applications.</p>
    </main>
    <footer class="bg-dark text-white text-center py-3">
        &copy; 2025 Robert Ashe. All rights reserved.
    </footer>
</body>
</html>

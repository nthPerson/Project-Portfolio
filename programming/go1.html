<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Unitree Go1 Software Package">
    <title>Unitree Go1 Software Package</title>
    <link rel="stylesheet" href="../bootstrap.min.sandstone.css">
    <style>
        .navbar-brand {
            font-size: 1.75rem;
            font-weight: bold;
        }
        .navbar-subtitle {
            font-size: 1rem;
            color: rgba(255, 255, 255, 0.8);
            display: block;
        }
        .btn-success, .btn-warning {
            margin: 0 10px;
        }
        img {
            display: block;
            margin: 20px auto;
            border-radius: 0.5rem;
            max-width: 100%;
        }
        video {
            display: block;
            margin: 20px auto;
            border-radius: 0.5rem;
            max-width: 100%;
        }
        figure {
            text-align: center;
        }
        figure figcaption {
            margin-top: 5px;
            font-style: italic;
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-lg bg-dark" data-bs-theme="dark">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">Robert Ashe - Project Portfolio
                <span class="navbar-subtitle">Big Data Analytics M.S., San Diego State University</span>
            </a>
        </div>
    </nav>
    <main class="container my-5">
        <h1 class="text-center">Unitree Go1 Software Package</h1>
        <p>The Go1 Software Package is a high-level control framework for Unitree quadruped robots, integrating seamlessly with Building Information Models (BIM) and Unity-based simulations to perform human activity recognition (HAR) in construction environments. This project was developed as part of a research initiative to explore the potential of robots in construction engineering.</p>
        <div class="text-center">
            <a href="https://github.com/nthPerson/UnitreeGO1-HRI-BIM" class="btn btn-success" target="_blank">GitHub Repository</a>
            <a href="https://drive.google.com/drive/folders/1wp_nPnqQrEC_u9uZVlHhcJBwAp9au4O_?usp=sharing" class="btn btn-warning" target="_blank">Image Gallery</a>
        </div>
        <h2 class="mt-5">My Role</h2>
        <p>I was responsible for implementing the explainable AI (XAI) feature, enabling the system to generate natural language explanations for AI inferences. My work included developing the core logic for the XAI processor, integrating it into the Python backend, and ensuring seamless communication between Unity, the robot, and other components.</p>
        <figure>
            <img src="../assets/go1_electronics_mount_process_1.jpg" alt="Go1 and post-processing equipment">
            <figcaption>Go1 and 3D printing post processing equipment during the creation of the mounting solution for the custom electronics used for human activity recognition.</figcaption>
        </figure>
        <h2>Technical Details</h2>
        <ul>
            <li>BIM integration for mission planning with Unity.</li>
            <li>Modular Python client-server architecture for scalability.</li>
            <li>Explainable AI feature for trust and transparency in human-robot collaboration.</li>
        </ul>
        <figure>
            <img src="../assets/go1_electronics_mount_process_6.jpg" alt="Raspberry Pi serving video streams">
            <figcaption>A view of the Raspberry Pi that serves the video stream to the local server that enables human activity recognition.</figcaption>
        </figure>
        <h2>Challenges and Solutions</h2>
        <p>Scaling up production for custom hardware mounts and ensuring real-time processing efficiency presented significant challenges. These were overcome by optimizing manufacturing processes and employing efficient data streaming techniques.</p>
        <figure>
            <video controls>
                <source src="../assets/slow_XAI_test.MOV" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Initial testing of the explainable AI (XAI) feature during development. A researcher (Robert Ashe) swings a hammer to simulate a construction task, and the system responds with an explanation of the inference.</figcaption>
        </figure>
    </main>
    <footer class="bg-dark text-white text-center py-3">
        &copy; 2025 Robert Ashe. All rights reserved.
    </footer>
</body>
</html>